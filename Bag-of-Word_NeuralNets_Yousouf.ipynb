{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = None\n",
    "\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re, os, sys, time, argparse\n",
    "\n",
    "### Hiding unwanted compile errors due to tensorflow not being built for specific\n",
    "### computing architecture.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def W(shape=None, name=None, mean=0.0, stddev=0.1, dtype=tf.float32, seed=None):\n",
    "    '''\n",
    "    Generates a trainable matrix with values following a normal distribution with specified mean and standard\n",
    "    deviation, values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    shape  : Int[]       -> (2, 1) array with the desired input&output sizes of the operation layer\n",
    "    name   : String      -> Weight matrix name\n",
    "    mean   : Float       -> mean value of distribution\n",
    "    stddev : Float       -> standard deviation of distribution\n",
    "    '''\n",
    "    name = name + \"_Truncated_Normal\"\n",
    "    initial = tf.truncated_normal(shape, mean, stddev, dtype, seed, name)\n",
    "    name = name + \"_Weight_Variable\"\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def B(shape=None, name=None, value=0.1, dtype=tf.float32, verify_shape=False):\n",
    "    '''\n",
    "    Generates and return a trainable bias matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    shape  : Int[]       -> (2, 1) array with the desired input&output sizes of the operation layer\n",
    "    name   : String      -> Weight matrix name\n",
    "    value  : Float       -> Initial value of matrix elements\n",
    "    '''\n",
    "    name = name + \"_Constant\"\n",
    "    initial = tf.constant(value, dtype, shape, name, verify_shape)\n",
    "    name = name + \"_Bias_Variable\"\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def Multiply(a, b, name=None):\n",
    "    '''\n",
    "    Returns the result of a matrix multiplication.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a      : tf.tensor\n",
    "    b      : tf.tensor \n",
    "    name   : String      -> Weight matrix name\n",
    "    '''\n",
    "    name = name + \"_Mat_Multiplication\"\n",
    "    return tf.matmul(a, b, name=name)\n",
    "    \n",
    "def Add(a, b, name=None):\n",
    "    '''\n",
    "    Returns the result of an element-wise matrix addition.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a      : tf.tensor\n",
    "    b      : tf.tensor \n",
    "    name   : String      -> Weight matrix name\n",
    "    '''\n",
    "    name = name + \"_Mat_Addition\"\n",
    "    return tf.add(a, b, name=name)\n",
    "\n",
    "def Drop(x, keep_prob, name=None):\n",
    "    name = name + \"_Dropout\"\n",
    "    return tf.nn.dropout(x, keep_prob, name=name)\n",
    "\n",
    "def Activation(features, activation, name=None):\n",
    "    name = name + \"_Activation_Function\"\n",
    "    if   (activation == \"ELU\"):\n",
    "        tmp  = tf.nn.elu(features, name)  \n",
    "    elif (activation == \"RELU\"):\n",
    "        tmp  = tf.nn.relu(features, name)\n",
    "    elif (activation == \"SIGMOID\"):\n",
    "        tmp  = tf.nn.sigmoid(features, name)\n",
    "    elif (activation == \"TANH\"):\n",
    "        tmp  = tf.nn.tanh(features, name) \n",
    "    elif (activation == \"SOFTPLUS\"):\n",
    "        tmp  = tf.nn.softplus(features, name)    \n",
    "    elif (activation == \"SOFTSIGN\"):\n",
    "        tmp  = tf.nn.softsign(features, name)\n",
    "    elif (activation == \"SOFTMAX\"):\n",
    "        tmp  = tf.nn.softmax(features, name=name)\n",
    "    else:\n",
    "        tmp  = features\n",
    "    return tmp\n",
    "\n",
    "def Pool(value, ksize=None, mode=\"MAX\", stride=None, name=None):\n",
    "    if (mode == \"MAX\"):\n",
    "        name = name + \"_Max_Pooling\"\n",
    "        tmp = tf.nn.max_pool(value, ksize, stride, \"SAME\", \"NHWC\", name)\n",
    "    elif (mode == \"MEAN\"):\n",
    "        name = name + \"_Mean_Pooling\"\n",
    "        tmp = tf.nn.avg_pool(value, ksize, stride, \"SAME\", \"NHWC\", name)\n",
    "    else:\n",
    "        tmp = value\n",
    "    return tmp\n",
    "\n",
    "def Conv2D(img, filtr, strides, padding, gpu, name=None):\n",
    "    name = name + \"_2D_Convolution\"\n",
    "    return tf.nn.conv2d(img, filtr, strides, padding, gpu, \"NHWC\", name)\n",
    "\n",
    "def BatchNorm(x, n_out, train_or_test, convo=False, name=None):\n",
    "    name = name + \"_Batch_Normalisation\"\n",
    "    beta  = B(value=0.0, shape=[n_out], name=name+\"beta\")\n",
    "    gamma = B(value=1.0, shape=[n_out], name=name+\"gamma\")\n",
    "    \n",
    "    if (convo == True):\n",
    "        m, v = tf.nn.moments(x, [0, 1, 2], name=name+'_Moments')\n",
    "    else:\n",
    "        m, v = tf.nn.moments(x, [0], name=name+'_Moments')\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "\n",
    "    def mean_var_with_update():\n",
    "        ema_apply_op = ema.apply([m, v])\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(m), tf.identity(v)\n",
    "\n",
    "    mean, var = tf.cond(train_or_test, mean_var_with_update, lambda: (ema.average(m), ema.average(v)))\n",
    "    xbn = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return xbn\n",
    "    \n",
    "def dense_2_onehot(labels_dense, num_classes):\n",
    "    num_labels     = labels_dense.shape[0]\n",
    "    index_offset   = np.arange(num_labels) * num_classes   \n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "    \n",
    "def onehot_2_dense(labels_onehot):\n",
    "    num_labels = np.shape(labels_onehot)[0]   \n",
    "    denses = []\n",
    "    for i in range(num_labels):\n",
    "        one_locations = np.where(np.equal(labels_onehot[i], 1))\n",
    "        denses.append(one_locations[0][0])\n",
    "    return denses\n",
    "\n",
    "def DenseLayer(Tensor, In, Out, Name = None, Function = \"ELU\", Dropout = None, Batchnorm = None):\n",
    "    Name = \"Dense_\" + Name \n",
    "    \n",
    "    Weight = W([In, Out], Name)\n",
    "    Bias   = B([Out], Name)\n",
    "    Output = Multiply(Tensor, Weight, Name)\n",
    "    Output = Add(Output, Bias, Name)\n",
    "    Output = Activation(Output, Function, Name)\n",
    "\n",
    "    if (Batchnorm != None):\n",
    "        Output = BatchNorm(Output, Out, Batchnorm, False, Name)\n",
    "\n",
    "    if (Dropout != None):\n",
    "        Output = Drop(Output, Dropout, Name)\n",
    "        \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProgressBar(object):\n",
    "    DEFAULT  = 'Progress: %(bar)s %(current)d/%(total)d (%(percent)3d%%)'\n",
    "    TRAINING = 'Training Epoch %(epoch)d: %(bar)s %(current)d/%(total)d (%(percent)3d%%)'\n",
    "    \n",
    "    def __init__(self, total, width=60, fmt=DEFAULT, symbol='=', output=sys.stderr):\n",
    "        self._total   = total\n",
    "        self._width   = width\n",
    "        self._symbol  = symbol\n",
    "        self._output  = output\n",
    "        self._fmt     = re.sub(r'(?P<name>%\\(.+?\\))d', r'\\g<name>%dd' % len(str(total)), fmt)\n",
    "        self._current = 0\n",
    "        self._epoch   = 0\n",
    "\n",
    "    def __call__(self, epoch, current):\n",
    "        self._current += current\n",
    "        self._epoch = epoch\n",
    "        percent = self._current / float(self._total)\n",
    "        size = int(self._width * percent)\n",
    "        remaining = self._total - self._current\n",
    "        bar = '[' + self._symbol * size + ' ' * (self._width - size) + ']'\n",
    "\n",
    "        args = {\n",
    "            'epoch'    : self._epoch,\n",
    "            'total'    : self._total,\n",
    "            'bar'      : bar,\n",
    "            'current'  : self._current,\n",
    "            'percent'  : percent * 100,\n",
    "            'remaining': remaining}\n",
    "        \n",
    "        ### uncomment if running outside of jupyter-notebook\n",
    "        #print('\\r' + self._fmt%args, file=self._output, end='')\n",
    "        ### comment if running outside of jupyter-notebook\n",
    "        print(self._fmt%args)\n",
    "\n",
    "    def done(self):\n",
    "        self._current = self._total\n",
    "        ### uncomment if running outside of jupyter-notebook\n",
    "        #print('', file=self._output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, images, labels, oneHot=False, oneHot_depth=None):\n",
    "        if (np.shape(images)[0] == np.shape(labels)[0]):\n",
    "            self._images = images\n",
    "            self._size   = len(images)\n",
    "            self._epochs = 0\n",
    "            self._index  = 0\n",
    "            self._onehot = oneHot\n",
    "            if (oneHot == True):\n",
    "                if (oneHot_depth > 0):\n",
    "                    self._labels = dense_2_onehot(labels, oneHot_depth)\n",
    "                # special case for train_test-valis_split()\n",
    "                elif (oneHot_depth == -1):\n",
    "                    self._labels = labels\n",
    "                else:\n",
    "                    print(\"Enter depth\")\n",
    "            else:\n",
    "                self._labels = labels\n",
    "        else:\n",
    "            print(\"Images and Labels must be of same lenght\")\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._size\n",
    "    \n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs\n",
    "\n",
    "    @property\n",
    "    def index(self):\n",
    "        return self._index\n",
    "\n",
    "    @property\n",
    "    def images(self, index=None):\n",
    "        return self._images\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "        \n",
    "    def get_images(self, index=None):\n",
    "        if (index == None):\n",
    "            return self._images\n",
    "        else:\n",
    "            return self._images[index]\n",
    "    \n",
    "    def get_labels(self, index=None):\n",
    "        if (index == None):\n",
    "            return self._labels\n",
    "        else:\n",
    "            return self._labels[index]\n",
    "\n",
    "    def imshow(self, index):\n",
    "        plt.figure()\n",
    "        if (self._onehot == True):\n",
    "            plt.title(onehot_2_dense([self._labels[index]]))\n",
    "        else:\n",
    "            plt.title(self._labels[index])\n",
    "        img = self._images[index][:,:,0]\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "    def train_test_valid_split(self, trn_size, tst_size, vld_size):\n",
    "        # Shuffle the data\n",
    "        if ((trn_size + tst_size +  vld_size) == 1.0):\n",
    "            perm = np.arange(self._size)\n",
    "            np.random.shuffle(perm)\n",
    "            self._images = self._images[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "\n",
    "            a = 0 + int(trn_size * self._size)\n",
    "            b = a + int(tst_size * self._size)\n",
    "            c = b + int(vld_size * self._size)\n",
    "            \n",
    "            training_set   = Dataset(self._images[0:a], self._labels[0:a], self._onehot, -1)\n",
    "            testing_set    = Dataset(self._images[a:b], self._labels[a:b], self._onehot, -1)\n",
    "            if (c != 0):\n",
    "                validation_set = Dataset(self._images[b:c], self._labels[b:c], self._onehot, -1)\n",
    "                return training_set, testing_set, validation_set\n",
    "            else:\n",
    "                return training_set, testing_set\n",
    "        else:\n",
    "            print(\"train_size:%3.2f + test_size:%3.2f must be equal to 1\" %(trn_size, tst_size))\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        tmp_x = self._size - self._index\n",
    "        if ((tmp_x % batch_size) != 0):\n",
    "            print(\"Batch of size %d cannot equally split set of size %d\" %(batch_size, tmp_x))\n",
    "        else:\n",
    "            start = self._index\n",
    "            self._index += batch_size\n",
    "\n",
    "            # Finished one epoch\n",
    "            if (self._index > self._size):  \n",
    "                self._epochs += 1  \n",
    "                # Shuffle the data\n",
    "                perm = np.arange(self._size)\n",
    "                np.random.shuffle(perm)\n",
    "                self._images = self._images[perm]\n",
    "                self._labels = self._labels[perm]\n",
    "                # Start next epoch\n",
    "                start = 0\n",
    "                self._index = batch_size\n",
    "                assert batch_size <= self._size\n",
    "            end = self._index\n",
    "            \n",
    "            return self._images[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, images, labels, logits, session):\n",
    "        self._session       = session\n",
    "        self._images        = images\n",
    "        self._labels        = labels\n",
    "        self._logits        = logits\n",
    "        self._loss          = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self._labels,  logits=self._logits, name='xentropy'), name='xentropy_mean')\n",
    "        \n",
    "        tf.summary.scalar(\"loss\", self._loss)\n",
    "        learning_rate_init = 1e-3 \n",
    "        global_step   = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate_init,\n",
    "                                                   global_step,\n",
    "                                                   decay_steps=30000,\n",
    "                                                   decay_rate=0.9,\n",
    "                                                   staircase=False,\n",
    "                                                   name='learning_rate_decay')\n",
    "        tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "        self._train_op      = tf.train.AdamOptimizer(learning_rate).minimize(self._loss, global_step)\n",
    "        self._accuracy      = tf.reduce_mean(tf.cast(tf.equal(\n",
    "                                                tf.argmax(self._logits, 1),\n",
    "                                                tf.argmax(self._labels, 1)),\n",
    "                                tf.float32))\n",
    "        self._saver         = tf.train.Saver()\n",
    "        self._summary       = tf.summary.merge_all()\n",
    "        self._writer        = tf.summary.FileWriter(\"./saved_models/\", self._session.graph)\n",
    "         \n",
    "        self._session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train_model(self, dataset, args, batch_size, epochs):\n",
    "        with self._session.as_default():\n",
    "            while (dataset.epochs_completed < epochs):        \n",
    "                progress = ProgressBar(dataset.size, fmt=ProgressBar.TRAINING)\n",
    "                for step in range(0, dataset.size, batch_size):\n",
    "                    imgs, lbls = dataset.next_batch(batch_size)\n",
    "                    dictionary = {self._images:imgs, self._labels:lbls}\n",
    "                    dictionary.update(args)\n",
    "                    \n",
    "                    self._train_op.run(feed_dict=dictionary)\n",
    "                    progress(dataset.epochs_completed, batch_size)\n",
    "                    \n",
    "                    ##### Write the summaries and print an overview fairly often.\n",
    "                    summary_str = self._session.run(self._summary, feed_dict=dictionary)\n",
    "                    self._writer.add_summary(summary_str, step)\n",
    "                    self._writer.flush()\n",
    "                    \n",
    "                progress.done()\n",
    "                \n",
    "                ##### Save a checkpoint and evaluate the model periodically.\n",
    "            checkpoint_file = os.path.join(\"./saved_models/\", 'model.ckpt')\n",
    "            self._saver.save(self._session, checkpoint_file, global_step=step)\n",
    "\n",
    "    def test_model(self, dataset, args, batch_size):\n",
    "        results = []\n",
    "        with tf.Graph().as_default():\n",
    "            for _ in range(0, dataset.size, batch_size):\n",
    "                imgs, lbls = dataset.next_batch(batch_size)\n",
    "                dictionary = {self._images:imgs, self._labels:lbls}\n",
    "                dictionary.update(args)\n",
    "                a = self._accuracy.eval(feed_dict=dictionary)\n",
    "                results.append(a)\n",
    "            b = np.mean(results)\n",
    "            print(\"Testing accuracy = %g\" %(b))\n",
    "\n",
    "    def predict(self, image, args, info, label=\"ONEHOT\"):\n",
    "        with tf.Graph().as_default():\n",
    "            img = np.reshape(image, [-1, info[\"Height\"], info[\"Width\"], info[\"Channels\"]])\n",
    "            dictionary = {self._images:img}\n",
    "            dictionary.update(args)\n",
    "            pred = self._session.run(self._logits, feed_dict=dictionary)\n",
    "            \n",
    "            if (label==\"DENSE\"):\n",
    "                tmp = np.arange(info[\"Classes\"])\n",
    "                tmp = pred\n",
    "                tmp = tmp / np.max(tmp)\n",
    "                tmp[np.where(tmp<np.max(tmp))] = 0\n",
    "                pred = onehot_2_dense(tmp)\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_architecture(l0_dense):\n",
    "    l1_in    = 3573\n",
    "    l1_out   = 50\n",
    "    l1_dense = DenseLayer(l0_dense, l1_in, l1_out, 'l1_dense', 'NONE')\n",
    "\n",
    "    #l2_in    = 3136\n",
    "    #l2_out   = 1568\n",
    "    #l2_dense = DenseLayer(l1_dense, l2_in, l2_out, 'l2_dense', 'ELU')\n",
    "\n",
    "    #l3_in    = 1568\n",
    "    #l3_out   = 784\n",
    "    #l3_dense = DenseLayer(l2_dense, l3_in, l3_out, 'l3_dense', 'SIGMOID')\n",
    "\n",
    "    #l4_in    = 784\n",
    "    #l4_out   = 392\n",
    "    #l4_dense = DenseLayer(l3_dense, l4_in, l4_out, 'l4_dense', 'RELU')\n",
    "\n",
    "    #l5_in    = 392\n",
    "    #l5_out   = 196\n",
    "    #l5_dense = DenseLayer(l4_dense, l5_in, l5_out, 'l5_dense', 'TANH')\n",
    "\n",
    "    l6_in    = 50\n",
    "    l6_out   = 1\n",
    "    l6_dense = DenseLayer(l1_dense, l6_in, l6_out, 'l6_dense', 'SOFTMAX')\n",
    "   \n",
    "    return l6_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    ##### Some basic information about the dataset.\n",
    "    Info     = {\"Height\":1, \"Width\":3573, \"Channels\":1, \"Classes\":1, \"Rows\": 124}\n",
    "    Elements = Info[\"Height\"] * Info[\"Width\"] * Info[\"Channels\"]\n",
    "    \n",
    "    ##### Loading features matrixs and target vector& Creating a dataset\n",
    "    imgs     = np.load(FLAGS.path + FLAGS.features)\n",
    "    lbls     = np.reshape(np.load(FLAGS.path + FLAGS.targets), [Info[\"Rows\"], Info[\"Classes\"]])\n",
    "    dataset = Dataset(imgs, lbls)\n",
    "                                                                \n",
    "    ##### Splitting the dataset into testing, training and validation datasets.\n",
    "    ###   Sum arguments to test_train_valid_split() should be 1.0\n",
    "    ###   dataset.train_test_valid_split(0.7, 0.3, 0.0) returns an empty validation set.\n",
    "    training_set, testing_set, validation_set = dataset.train_test_valid_split(0.5, 0.5, 0.00)\n",
    "\n",
    "    ##### Placeholders for images and labels\n",
    "    images   = tf.placeholder(dtype=tf.float32, shape=[None, Elements], name=\"Features\")\n",
    "    labels   = tf.placeholder(dtype=tf.float32, shape=[None, Info[\"Classes\"]],        name=\"Targets\")\n",
    "\n",
    "    ##### Model Architecture\n",
    "    logits = model_architecture(images)\n",
    "    \n",
    "    ##### Model.\n",
    "    session  = tf.Session(config=tf.ConfigProto())\n",
    "    model    = Model(images, labels, logits, session)\n",
    "\n",
    "    ##### Training and Testing model.\n",
    "    with session.as_default():\n",
    "        start = time.time()\n",
    "        args  = {}\n",
    "        model.train_model(training_set, args, batch_size=FLAGS.batch_size, epochs=FLAGS.epochs)\n",
    "        end   = time.time()\n",
    "    print(\"Time taken = %3.1fs\" %(end-start))\n",
    "    \n",
    "    with session.as_default():\n",
    "        args  = {}\n",
    "        model.test_model(testing_set, args, batch_size=FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/MuR/barista/MillionSong\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BagOfWord_Analysis.ipynb\r\n",
      "Bag-of-Word_NeuralNets_Yousouf.ipynb\r\n",
      "Bag-of-Word_Prepare_NeuralNets_Yousouf.ipynb\r\n",
      "features.npy\r\n",
      "HDF5_file_structure.pdf\r\n",
      "label.npy\r\n",
      "LICENSE\r\n",
      "lyrics.db\r\n",
      "Music_Analysis_Lauren.ipynb\r\n",
      "MusicExploration_Lauren.ipynb\r\n",
      "Music_exploration_Mike.ipynb\r\n",
      "Music_exploration_PierreYves.ipynb\r\n",
      "new_lyrics.npy\r\n",
      "one_hot_genres.csv\r\n",
      "one_hot_genres_stats.csv\r\n",
      "pivoted_table_1000songs.npy\r\n",
      "README.md\r\n",
      "saved_models\r\n",
      "Track_analysis.csv\r\n",
      "Track_analysis.xlsx\r\n",
      "Track_metadata.csv\r\n",
      "Track_musicbrainz.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"/home/MuR/barista/MillionSong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of size 8 cannot equally split set of size 62\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-03e7504cd56f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0munparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-afd044d516f6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0margs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mend\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time taken = %3.1fs\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2b50ccb28c0c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, dataset, args, batch_size, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mProgressBar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlbls\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs'     , type=int, default=50)\n",
    "parser.add_argument('--batch_size' , type=int, default=8)\n",
    "parser.add_argument('--device'     , type=str, default=\"/cpu:0\")\n",
    "parser.add_argument('--path'       , type=str, default=\"/home/MuR/barista/MillionSong/\")\n",
    "parser.add_argument('--features'   , type=str, default=\"features.npy\")\n",
    "parser.add_argument('--targets'    , type=str, default=\"label.npy\")\n",
    "parser.add_argument('--mode'       , type=str, default=\"TRAIN\")\n",
    "parser.add_argument('--logdir'     , type=str, default=\"./saved_models/model.ckpt-60\")\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "tf.reset_default_graph()\n",
    "main(argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
